---
title: "The Effect of Monetary Policy with Banks Deposit in Australia"
author: "Eungwon Lee"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.**  This research investigates the relationship between monetary policy, banks' deposit, and credit  in Australia. By incorporating the deposit channel in the analysis, it is found that changes in deposits play a role in transmitting monetary policy effects on credit and real economy. The study shows that credit and real GDP respond more sensitively to monetary policy shocks when deposits are included in the model. This finding emphasizes the importance of considering the deposit channel in understanding the impact of monetary policy on credit and other macro variables in Australia.





>
> **Keywords.** SVAR, monetary policy, credit, deposit

#  The objective, question, and motivation

## Objective

The research objective is to investigate the relationship among monetary policy, banks deposit and credit growth, while considering other important macroeconomic variables in Australia, using a structural vector autoregression model. The overall question to be answered can thus be described as; How does the monetary policy and deposit growth affect the banks' credit growth in Australia?


## Motivation

The reason why the objective question is of interest is that banks' credit plays an important role in the economy as it supports investment for firms, consumption for households, and sometimes causes inflationary pressure. Thus, the effect of monetary policy on credit has been deemed to be an important concern for researchers and central bankers. A channel that explains how monetary policy works through credit is called the credit channel. In the credit channel, changes in monetary policy affect the required reserves of banks, which cause banks to change the quantity of loans they provide (e.g., @bernanke1992federal; @jimenez2014hazardous). However, the effect of changes in reserves has been estimated to be too small to exert a meaningful influence on bank lending (@woodford2010financial).

One suggestion to fill this gap is proposed by @drechsler2017deposits. In the paper, they argue that banks' credit substantially reacts to monetary policy through changes in deposits. According to their estimation, it accounts for the entire transmission of monetary policy through the bank's balance sheet. I incorporate this deposit channel in the SVAR framework to evaluate the effect of monetary policy with deposit change on banks' credit. 


#  Data and their properties

Following @berkelmans2005credit, variables are largely divided into two groups: the external and domestic sectors. For the external sector, real commodity prices (such as oil prices) and real U.S. GDP are used. The domestic sector is captured by real Australian GDP, inflation rate, banks' credit, deposit, US exchange rate, and cash rate.

At the moment, I load the related packages for the data work as follows:


```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide
rm(list=ls())
```

```{r}
#| echo: true
#| message: false
library(readrba)
library(readabs)
library(fredr)
library(tseries)
library(ggplot2)
library(cowplot)
library(dplyr)
library(zoo)
library(xts)
library(lubridate)
library(tidyr)
library(tibble)
library(lattice)
library(gridExtra)
library(FinTS)


```


The external sector is included considering Australia's status as a small open economy that heavily relies on foreign economic conditions. Commodity prices are included as they are known to capture the global business cycle that affects the domestic economy, and due to the fact that they usually ensure that the impulse responses does not show a price puzzle. The Index of Commodity Prices Australia (ICP) will be used for commodity prices. ICP is a measure of the average change in prices of Australia's major export commodities over time using price data from a basket of commodities, including minerals, agricultural products, and energy resources. The ICP is a key indicator of Australia's terms of trade. The inclusion of the U.S. GDP is also important as it has been Australia's one of the major trading partners and the largest economy that influence the world economy heavily.

For the domestic sector, real GDP is included to capture the level of domestic economic activity, while the inflation rate is included as the Reserve Bank of Australia (RBA) has adopted an inflation target since 1990, which strongly affects monetary policy. Banks' credit and deposit are key variables in the model as they form contemporaneous relationship with monetary policy. Finally, the cash rate is the primary tool of monetary policy for the RBA, and it is therefore an essential variable to include in the model. 

Quarterly data between Q1 1990 and Q2 2019 was acquired from the Reserve Bank of Australia (RBA) and the Australian Bureau of Statistics (ABS) using the "readrba" and "readabs" packages in R. The data were mostly transformed into logarithmic form, except for exchange rate and cash rate, allowing the coefficients to be interpreted as elasticities. The exchange rate is presented AUD/USD, while the cash rate is presented in percentage.

Unfortunately, I cannot use data prior to Q1 1990 as RBA adopted the cash rate as its primary policy tool in the early 1990s, replacing the "interest rate on 90-day bills" as the target for monetary policy. Additionally, I cannot use data after Q2 2019 due to a break in the credit data at that time.

```{r}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# test

# I would suggest that you merged all the data into one table, such that you can call them easier. Doing this would also enable you to do loops over the variables. I have attached my own merge, which you can use if interested. You could then do ADF, ACF and plot using a loop over the variables in the table! I have included my own code of ADF if interested (I know that the code could be more efficiently written). 

#Merging the series into vector Y
#Y = na.omit(merge(tindp, tcpi, texpec, tlend, thp, tfci))
#colnames(Y)<- c("indu", "cpi", "exp", "lend","hp","fci")

# Code for ADF
#max_lag = 12
#adf_ <- list()
#for (i in 1:6) {
#  adf_result = adf.test(Y[,i], k = max_lag)
#  adf_[[i]] <- adf_result
#}
#head(adf_)

# View the ADF test results
#summary(adf_result)

#adf_table <- data.frame(Test_Statistic = numeric(length(adf_)), 
#                        p_value = numeric(length(adf_)), 
#                        Lags_Used = numeric(length(adf_)))

# Fill in the data frame with the test results
#for (i in 1:length(adf_)) {
#  adf_table[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
#  adf_table[i, "p_value"] = round(adf_[[i]]$p.value,3)
#  adf_table[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
#}
```



```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# load data from RBA and FRB
temp_ex_comp          <- read_rba(series_id = "GRCPAIAD")
                         fredr_set_key("7afc2e18e3cf3585055851ad2f310f53")
temp_ex_usrealgdp     <- fredr(series_id = "GDPC1")

temp_dm_realgdp       <- readrba::read_rba(series_id = "GGDPCVGDP")
temp_dm_price         <- readrba::read_rba(series_id = "GCPIAG")
temp_dm_credit        <- readrba::read_rba(series_id = "DLCACN")
   temp_M3            <- readrba::read_rba(series_id = "DMAM3N")
   temp_currency      <- readrba::read_rba(series_id = "DMACN")
   value              <- temp_M3$value - temp_currency$value
temp_dm_deposit       <- cbind(temp_M3[,1], value, temp_M3[,4])
temp_dm_us_ex_rate    <- readrba::read_rba(series_id = "FXRUSD")
temp_dm_cashrate      <- read_cashrate(type = c("target"))

# change data to quarterly
ex_comp          <- to.quarterly(xts(temp_ex_comp$value, temp_ex_comp$date), OHLC = FALSE)
ex_usrealgdp     <- to.quarterly(xts(temp_ex_usrealgdp$value, temp_ex_usrealgdp$date), OHLC = FALSE)
dm_realgdp       <- to.quarterly(xts(temp_dm_realgdp$value, temp_dm_realgdp$date), OHLC = FALSE)
dm_price         <- to.quarterly(xts(temp_dm_price$value, temp_dm_price$date), OHLC = FALSE)
dm_credit        <- to.quarterly(xts(temp_dm_credit$value, temp_dm_credit$date), OHLC = FALSE)
dm_deposit       <- to.quarterly(xts(temp_dm_deposit$value, temp_dm_deposit$date), OHLC = FALSE)
dm_us_ex_rate    <- to.quarterly(xts(temp_dm_us_ex_rate$value, temp_dm_us_ex_rate$date), OHLC = FALSE)
dm_cashrate      <- to.quarterly(xts(temp_dm_cashrate$value, temp_dm_cashrate$date), OHLC = FALSE) 

# unify date so that I can merge with other data
index(ex_usrealgdp) <- seq(as.Date("1947-03-01"), by = "3 months", length.out = nrow(ex_usrealgdp))
index(dm_realgdp)   <- seq(as.Date("1959-09-01"), by = "3 months", length.out = nrow(dm_realgdp))
index(dm_price)     <- as.yearqtr(index(dm_price))
index(dm_realgdp)   <- as.yearqtr(index(dm_realgdp))
index(ex_usrealgdp) <- as.yearqtr(index(ex_usrealgdp))

# merge original data
merged_data  <- na.omit(merge(ex_comp, ex_usrealgdp, dm_realgdp, dm_price, dm_credit, dm_deposit, dm_us_ex_rate, dm_cashrate))
                           
# transform original data into log
log_merged_data <- log(merged_data)

# merge original and log transformed data for estimation
# dm_us_ex_rate, dm_cashrate are original, others are log transformed.
y           = cbind(log_merged_data[,1],
                    log_merged_data[,2],
                    log_merged_data[,3],
                    log_merged_data[,4],
                        merged_data[,8],
                    log_merged_data[,6],
                    log_merged_data[,5],
                        merged_data[,7]
                    )


```






Below is how data look like. As we see below, stock and flow variables mostly have upward trend and it disappears when I take difference. Some variables such as GDP and deposit exhibit spark around 2020 as below.

###### Figure 1. Time series of log variables from 1990 Q1 to 2019 Q2 except USD exchange rate and cash rate is not {style="text-align: center;"}


```{r, warning=FALSE, message=FALSE}

#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from Thomas Kronholm Moeller.

pl_ex_comp = ggplot(data = log_merged_data[,1], aes(x = index(log_merged_data[,1]), y = log_merged_data[,1])) +
  geom_line(color = "black") +
  labs(title = "Commodity Price", x = "Year", y = "Ln Commodity Price") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_ex_usrealgdp = ggplot(data = log_merged_data[,2], aes(x = index(log_merged_data[,2]), y = log_merged_data[,2])) +
  geom_line(color = "black") +
  labs(title = "US Real GDP", x = "Year", y = "Ln US Real GDP") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_realgdp = ggplot(data = log_merged_data[,3], aes(x = index(log_merged_data[,3]), y = log_merged_data[,3])) +
  geom_line(color = "black") +
  labs(title = "Australia Real GDP", x = "Year", y = "Ln Au Real GDP") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_price = ggplot(data = log_merged_data[,4], aes(x = index(log_merged_data[,4]), y = log_merged_data[,4])) +
  geom_line(color = "black") +
  labs(title = "Consumer Price Index", x = "Year", y = "Ln Index") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_credit = ggplot(data = log_merged_data[,5], aes(x = index(log_merged_data[,5]), y = log_merged_data[,5])) +
  geom_line(color = "black") +
  labs(title = "Bank Total Credit", x = "Year", y = "Ln Credit") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_deposit = ggplot(data = log_merged_data[,6], aes(x = index(log_merged_data[,6]), y = log_merged_data[,6])) +
  geom_line(color = "black") +
  labs(title = "Bank Deposit", x = "Year", y = "Ln Deposit") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_us_ex_rate = ggplot(data = merged_data[,7], aes(x = index(merged_data[,7]), y = merged_data[,7])) +
  geom_line(color = "black") +
  labs(title = "US Exchange Rate", x = "Year", y = "AUD/USD") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

pl_dm_cashrate = ggplot(data = merged_data[,8], aes(x = index(merged_data[,8]), y = merged_data[,8])) +
  geom_line(color = "black") +
  labs(title = "Cash Rate", x = "Year", y = "Cash Rate (%)") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()


grid.arrange(pl_ex_comp, pl_ex_usrealgdp, pl_dm_realgdp, pl_dm_price, pl_dm_credit, pl_dm_deposit, pl_dm_us_ex_rate, pl_dm_cashrate, nrow = 3, ncol = 3)

```

# 

Once we take the first difference between quarters, we observe that the trend disappears from most of the data. However, we still observe that first differenced banks credit and deposit seems to exhibit some trend. At the same time, we see that variances are still quite different across time in most of the variables.



###### Figure 2. Time series of log differenced variables from 1990 Q1 to 2019 Q2 {style="text-align: center;"}


```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from Thomas Kronholm Moeller.

# difference quarter on quarter
dff_merged_data <- na.omit(merged_data - lag(merged_data))
dff_log_merged_data <- na.omit(log_merged_data - lag(log_merged_data))


dff_pl_ex_comp = ggplot(data = dff_log_merged_data[,1], aes(x = index(dff_log_merged_data[,1]), y = dff_log_merged_data[,1])) +
  geom_line(color = "black") +
  labs(title = "Commodity Price", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_ex_usrealgdp = ggplot(data = dff_log_merged_data[,2], aes(x = index(dff_log_merged_data[,2]), y = dff_log_merged_data[,2])) +
  geom_line(color = "black") +
  labs(title = "US Real GDP", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_realgdp = ggplot(data = dff_log_merged_data[,3], aes(x = index(dff_log_merged_data[,3]), y = dff_log_merged_data[,3])) +
  geom_line(color = "black") +
  labs(title = "Australia Real GDP", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_price = ggplot(data = dff_log_merged_data[,4], aes(x = index(dff_log_merged_data[,4]), y = dff_log_merged_data[,4])) +
  geom_line(color = "black") +
  labs(title = "Consumer Price Index", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_credit = ggplot(data = dff_log_merged_data[,5], aes(x = index(dff_log_merged_data[,5]), y = dff_log_merged_data[,5])) +
  geom_line(color = "black") +
  labs(title = "Bank Total Credit", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_deposit = ggplot(data = dff_log_merged_data[,6], aes(x = index(dff_log_merged_data[,6]), y = dff_log_merged_data[,6])) +
  geom_line(color = "black") +
  labs(title = "Bank Deposit", x = "Year", y = "Ln Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_us_ex_rate = ggplot(data = dff_merged_data[,7], aes(x = index(dff_merged_data[,7]), y = dff_merged_data[,7])) +
  geom_line(color = "black") +
  labs(title = "US Exchange Rate", x = "Year", y = "Difference") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

dff_pl_dm_cashrate = ggplot(data = dff_merged_data[,8], aes(x = index(dff_merged_data[,8]), y = dff_merged_data[,8])) +
  geom_line(color = "black") +
  labs(title = "Cash Rate", x = "Year", y = "Difference (%p)") +
  scale_x_continuous(limits = c(1990, 2019))+
  theme_minimal()

grid.arrange(dff_pl_ex_comp, dff_pl_ex_usrealgdp, dff_pl_dm_realgdp, dff_pl_dm_price, dff_pl_dm_credit, dff_pl_dm_deposit, dff_pl_dm_us_ex_rate, dff_pl_dm_cashrate, nrow = 3, ncol = 3)

```
#

Autocorrelation Function (ACF) plots show the correlation between a time series and its lagged values. Stock variables such as credit and deposit exhibit the most strong correlation in time series while exchange rate and cash rate show relatively weak correlation between periods.

###### Figure 3. Autocorrelation function (ACF) plots (all log term except exchange rate and cash rate) {style="text-align: center;"}


```{r ACF plots, echo = FALSE}

# This code is from Ray Gomez.

par(mfrow=c(2,4))
a_ex_comp = acf(log_merged_data[,1], main = "ACF, Commodity Pice", ylab = "Autocorrelation", type = "correlation")
a_ex_usrealgdp = acf(log_merged_data[,2], main = "ACF, US Real GDP", ylab = "Autocorrelation", type = "correlation")
a_dm_realgdp = acf(log_merged_data[,3], main = "ACF, AU Real GDP", ylab = "Autocorrelation", type = "correlation")
a_dm_price = acf(log_merged_data[,4],  main = "ACF, CPI", ylab = "Autocorrelation", type = "correlation")
a_dm_credit = acf(log_merged_data[,5], main = "ACF, Total Credit", ylab = "Autocorrelation", type = "correlation")
a_dm_deposit = acf(log_merged_data[,6], main = "ACF, Total Deposit", ylab = "Autocorrelation", type = "correlation")
a_dm_us_ex_rate = acf(merged_data[,7],  main = "ACF, Exchange Rate", ylab = "Autocorrelation", type = "correlation")
a_dm_cashrate = acf(merged_data[,8],  main = "ACF, Cash Rate", ylab = "Autocorrelation", type = "correlation")
```
#
The Partial Autocorrelation Function (PACF) is the correlation between a time series and a lagged version of itself, controlling for the values of the intermediate lags. The PACF plot shows the partial correlation coefficients between the time series and its lags. These coefficients represent the correlation between the time series and the lagged values of itself, with the effect of the intermediate lags removed. We see that for most of the variables, first lag is an important predictor of the current value of the time series. 

###### Figure 4. Partial Autocorrelation function (PACF) plots {style="text-align: center;"}

```{r PACF plots, echo = FALSE}

# This code is from Ray Gomez

par(mfrow=c(2,4))
pa_ex_comp = pacf(log_merged_data[,1], main = "PACF, Commodity Pice")
pa_ex_usrealgdp = pacf(log_merged_data[,2], main = "PACF, US Real GDP")
pa_dm_realgdp = pacf(log_merged_data[,3], main = "PACF, AU Real GDP")
pa_dm_price = pacf(log_merged_data[,4],  main = "PACF, CPI")
pa_dm_credit = pacf(log_merged_data[,5], main = "PACF, Total Credit")
pa_dm_deposit = pacf(log_merged_data[,6], main = "PACF, Total Deposit")
pa_dm_us_ex_rate = pacf(merged_data[,7],  main = "PACF, Exchange Rate")
pa_dm_cashrate = pacf(merged_data[,8],  main = "PACF, Cash Rate")
```




## Integration order verification

I conduct Augmented Dickey-Fuller test for log transformed variables and original cash rate and exchange rate using the "adf()" function to examines the presence of a unit root in the time series. The results of the test are displayed in the table below. Across all variables except exchange rate, I cannot not reject the null hypothesis of unit root. It indicates that the time series is non-stationary and requires differencing to achieve stationarity.


```{r ADF test1}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from Thomas Kronholm Moeller.
#ADF test

adf_ <- list()
for (i in 1:8) {
  adf_result = adf.test(y[,i], k = 4)
  adf_[[i]] <- adf_result
}

head(adf_)

# View the ADF test results
summary(adf_result)

adf_table <- data.frame(Test_Statistic = numeric(length(adf_)), 
                        p_value = numeric(length(adf_)), 
                        Lags_Used = numeric(length(adf_)))

# Fill in the data frame with the test results
for (i in 1:length(adf_)) {
  adf_table[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
  adf_table[i, "p_value"] = round(adf_[[i]]$p.value,3)
  adf_table[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
}
```

###### Table 1. Augmented Dickey-Fuller test for log transformed variables except ex_rate and cash rate {style="text-align: center;"}
```{r ADF test1 print}
#| echo: false
#| message: false
#| warning: false


# Print the data frame
rownames(adf_table)<- c("Commodity price", "US real GDP", "AU Real GDP", "CPI","Cash rate","Banks' deposit", "Banks' credit", "USD exchange rate")
colnames(adf_table)<- c("Test statistic", "P-value", "Lags")
print(adf_table)


```

#

Then I conduct Augmented Dickey-Fuller test for first differenced log transformed variables and original cash rate and exchange rate using the "adf()" function to examines the presence of a unit root in the differenced time series. The results of the test are displayed in the table below. As we can expect, most of variables except banks' deposit and credit, P-values of the tests are below 5% or 1%, implying the rejection of null hypothesis of unit root. 


```{r ADF test2}
#| echo: false
#| message: false
#| warning: false
#| results: hide


# This code is from Thomas Kronholm Moeller.

# compute dfferenced variables
dff_y           = cbind(dff_log_merged_data[,1],
                         dff_log_merged_data[,2],
                         dff_log_merged_data[,3],
                         dff_log_merged_data[,4],
                             dff_merged_data[,8],
                         dff_log_merged_data[,6],
                         dff_log_merged_data[,5],
                             dff_merged_data[,7]
                  
                    
)

# ADF test
dff_adf_ <- list()
for (i in 1:8) {
  dff_adf_result = adf.test(dff_y[,i], k = 4)
  dff_adf_[[i]] <- dff_adf_result
}

head(dff_adf_)

# View the ADF test results
summary(dff_adf_result)

dff_adf_table <- data.frame(Test_Statistic = numeric(length(dff_adf_)), 
                        p_value = numeric(length(dff_adf_)), 
                        Lags_Used = numeric(length(dff_adf_)))

# Fill in the data frame with the test results
for (i in 1:length(dff_adf_)) {
  dff_adf_table[i, "Test_Statistic"] = round(dff_adf_[[i]]$statistic,3)
  dff_adf_table[i, "p_value"] = round(dff_adf_[[i]]$p.value,3)
  dff_adf_table[i, "Lags_Used"] = round(dff_adf_[[i]]$parameter,3)
}

```

###### Table 2. Augmented Dickey-Fuller test for quarter on quarter differenced variables {style="text-align: center;"}
```{r ADF test2 print}
#| echo: false
#| message: false
#| warning: false


# Print the data frame
rownames(dff_adf_table)<- c("Commodity price", "US real GDP", "AU Real GDP", "CPI","Cash rate","Banks' deposit", "Banks' credit", "USD exchange rate")
colnames(dff_adf_table)<- c("Test statistic", "P-value", "Lags")
print(dff_adf_table)


```

This raises the issue of the appropriate estimation methodology. Some papers (i.g. Berkelmans, 2005) estimate SVAR in levels even when the variables are I(1) while others (Siami-Namini, 2016) use first differences in their model.



# The model and hypothesis

## The model

The structural VAR model with $4$ lags is given by

$$
\begin{align}
  B_0y_t &= b_0 + B_1 y_{t-1} + \dots + B_4 y_{t-4} + u_t \\
  u_{t}| Y_{t-1} &\sim {iid} ( 0_N, I_N)
\end{align}
$$
Where $B_0$ is a matrix of contemporaneous relationships, $y_t$ is a vector of endogenous variables, $y_{t-1}$, $\dots$, $y_{t-1}$ are vectors of lags, and $u_t$ is a vector of independent structural shocks.

$y_t$ contains eight variables: commodity prices ($com_t$), the U.S. real GDP ($usgdp_t$), domestic real GDP ($gdp_t$), inflation rate($\pi_t$),  cash rate ($i_t$), banks' deposit($d_t$), banks' credit($c_t$), and USD exchange rate ($ex_t$).

Premultiplying $B_0^{-1}$ to the structural VAR model gives us the reduced form representation as follows:

$$
\begin{align}
  y_t &= \mu_0 + A_1 y_{t-1} + \dots + A_4 y_{t-4} + \epsilon_t \\
  \epsilon_{t}| Y_{t-1} &\sim {iid} ( 0_N, \Sigma)
\end{align}
$$
where $A_i=B_0^{-1}B_i$, $\epsilon_t=B_0^{-1}u_t$, and $\Sigma=$$B_0^{-1}B_0^{-1 \prime}$

Using the reduced form equations, we can estimate $\Sigma$. However $\Sigma$ has $8(8+1)/2=36$ unique elements, which is the number of equations, yet we have $8^2=64$ unknowns in $B_0$. Hence I impose exclusion restrictions on $B_0$ to identify the system.


# Hypothesis
I want to estimate how monetary shocks affect credit while controlling for deposits. I expect that credit will be less responsive to monetary policy when deposits are included as an endogenous variable. This expectation is consistent with the deposit channel of monetary policy, where banks' credit is heavily influenced by changes in deposits in response to monetary policy.


# Identiying assumptions: case 1

In this case, I apply a lower triangular matrix to $B_0$ matrix.

Assumptions for $B_0$ matrix is as follows:

- Since Australian economy is a small open economy, I assume that the domestic variables have no effect on external variables while shocks from the external sector have simultaneous effect on the domestic economy.
- GDP is affected from the external sector but not affected by other domestic variables contemporaneously 
- Inflation is affected from the external sector and GDP but not affected by other domestic variables contemporaneously.
- Cash rate is affected from the external sector, GDP and inflation but not affected by other domestic variables contemporaneously.
- Deposit is affected from the external sector, GDP, inflation, and cash rate but not affected by other domestic variables contemporaneously. I assume that profit maximization motive makes banks to adjust the deposit rate at the same period that RBA adjusts the cash rate. And deposit volume changes accordingly.
- Credit is affected from the external sector, GDP, inflation, cash rate, and deposit but not affected by other domestic variables contemporaneously. Since the deposit is the most stable and major funding source for banks, I assume that banks reduces credit outstanding in response to deposit change.
- The exchange rate is affected from all variables contemporaneously.


Following equation summarizes the contemporaneous relationships among the variables:


$$
B_0y_t =
\begin{bmatrix}
a_{11} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
a_{21} & a_{22} & 0 & 0 & 0 & 0 & 0 & 0 \\
a_{31} & a_{32} & a_{33} & 0 & 0 & 0 & 0 & 0 \\
a_{41} & a_{42} & a_{43} & a_{44} & 0 & 0 & 0 & 0 \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} & 0 & 0 & 0 \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} & a_{66} & 0 & 0 \\
a_{71} & a_{72} & a_{73} & a_{74} & a_{75} & a_{76} & a_{77} & 0 \\
a_{81} & a_{82} & a_{83} & a_{84} & a_{85} & a_{86} & a_{87} & a_{88} \\
\end{bmatrix}
\begin{bmatrix}
com_t \\
usgdp_t \\
gdp_t \\
\pi_t \\
i_t \\
deposit_t \\
credit_t \\
ex_t \\
\end{bmatrix}
$$


Note that this assumption and thus order of the variables in $y_t$ is subject to change over the course of the research upon any better identifying assumptions. One problem that arises from using quarterly data instead of monthly is difficulty of setting identifying assumptions. When it is monthly data, we can utilize the assumption of informational delay. When deciding on monetary policy in the Australian economy, RBA takes into consideration a range of variables including, but are not limited to, CPI and GDP. However, due to informational delay, CPI, GDP, and other relevant variables may not be immediately available to the monetary authorities. This allows us to claim no contemporaneous relation between policy rate and some variables. This is difficult to argue in quarterly data since a quarter may be enough time to have contemporaneous relation between policy rate and some other variables.





```{r results for case 1}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from ECOM90007 L12 codes provided by Tomasz Woźniak.

library(mvtnorm)
library(plot3D)
library(HDInterval)
set.seed(123456)

# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
purple = "#b02442"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)

# setup
############################################################
N       = 8
p       = 4
S       = 50000
#h      = 8
h       = 10

# create Y and X
############################################################
y       = ts(y)
Y       = ts(y[5:118,], start=c(1991,1), frequency=4)
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:118-i,])
}


t0          = proc.time() # read processor time

# MLE
############################################################
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

```

## The prior distibution

The prior parameters are set as follows: $\kappa_1$, which is the overall shrinkage parameter to control the dispersion around the prior mean, is set to 1. $\kappa_2$, the overall shrinkage for the constant term, is set to 100. $\kappa_3$ is set to 1.

```{r}
#| echo: true
#| message: false

# Parameters for prior distribution 
############################################################
kappa.1        = 1^2
kappa.2        = 100
kappa.3        = 1
A.prior        = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:9,]  = kappa.3*diag(N)
V.prior        = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior        = diag(diag(Sigma.hat))
nu.prior       = N+1
```

## The posterior distibution

Using the likelihood and the prior distribution I obtain the posterior as follows:
```{r results for case 1_contd}
#| echo: true
#| message: false

# normal-inverse Wishard posterior parameters
############################################################
V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)

# posterior draws 
############################################################
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
B.posterior       = array(NA,c(N,N,S))
L                 = t(chol(V.bar))
for (s in 1:S){
  cholSigma.s     = chol(Sigma.posterior[,,s])
  B.posterior[,,s]= t(cholSigma.s)
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
}
```


## Empirical Investigation for baseline model (case 1)

As we see the Impulse Response Functions (IRFs) below, upon a cash rate shock, Australian real GDP decreases for more than a year and the impact statistically becomes zero in a longer term, deposit increases for nearly two years and then the impact becomes statistically zero in a longer term, price increases and then the effect gradually becomes zero, credit, which is the variable of my interest, increases in the short term for about a year and then decreases and the effect becomes statistically zero, and exchange rate increases shortly and then becomes statistically indistinguishable from zero.

```{r results for case 1_contd2}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# Impulse response functions
# Forecast Error Variance Decomposition
############################################################
IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S){
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    tmp = B.posterior[,,s]
    if (i==1){
      IRF.posterior[,,i,s]        = tmp
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% tmp
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior

t1          = proc.time()
(t1-t0)[3]/60 # Time of computations in minutes

# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1.RData")


# Plots of responses to domestic monetary policy shock
############################################################
load("irf-fevd-k1.RData")
IRFs.k1           = apply(IRF.posterior[3:8,5,,],1:2,mean)
IRFs.inf.k1       = apply(IRF.inf.posterior[3:8,5,],1,mean)
rownames(IRFs.k1) = colnames(Y)[3:8]

IRFs.k1.hdi    = apply(IRF.posterior[3:8,5,,],1:2,hdi, credMass=0.68)
hh             = 1:11
```





###### Figure 5. IRF to cash rate shock for $\kappa_1$ = 1 plots {style="text-align: center;"}

```{r irf-au-mps plots, echo = FALSE}

# irf-au-mps plots

par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:6){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,2,6,11),c("","1 quarter","5 quarter","10 quarter"))
  } else {
    axis(1,c(1,2,6,11),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
  
}

```


#
The contribution of the shocks to the mean square forecast error (MSFE) of the credit for 10 quarters ahead is given below. Specifically, it provides a sense of the relative importance of shocks in explaining the forecast variability of credit quantifying the percentage of the forecast error variance of credit that can be attributed to its own shocks and the shocks of other variables such as cash rate. Initially, its own shock contribute the most of the variance. However, as it proceeds, the role of its own shock shrinks considerably while other variables, especially cash rate (mps in the figure) explains much more of the variance as time goes by.

###### Figure 6. FEVD of banks' credit plots {style="text-align: center;"}


```{r fevd-credit plots, echo = FALSE}
# Plots of FEVD of Australian banks credit
############################################################
load("irf-fevd-k1.RData")
hh            = 1:(h+1)
fevd.au.credit= apply(FEVD.posterior[7,,,],1:2,mean)
fevd.au.credit= rbind(rep(0,h+1),apply(fevd.au.credit,2,cumsum))

colors = c("deepskyblue1","deepskyblue2","maroon1","maroon","maroon2","magenta","maroon3","maroon4")


par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,fevd.au.credit[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
axis(1,hh,c("","1 quarter","","","","5 quarter","","","","","10 quarter"))
axis(2,c(0,50,100),c("","FEVD[au.credit]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(fevd.au.credit[n,hh],fevd.au.credit[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(fevd.au.credit[1:8,7]+fevd.au.credit[2:9,7]))[c(5,7)], c("mps","crd"))

```






# Identiying assumptions: case 2(Long-run restriction)

In case 2, I additionally apply a long-run restriction to the baseline model (case 1 model). The restriction that I additionally impose is that monetary policy has no effect on the Australian real GDP in the long run. Since the algorithm (Algorithm 1 in lecture 13) I use works only for exactly identified models, the restrictions of interest to be imposed on the system must exactly identify the model. Hence, to make a room for a long-run restriction, I allow monetary policy to have an effect on the Australian real GDP contemporaneously and then I impose a restriction that the monetary policy has no effect on the Australian real GDP in the long run.

Following equation summarizes the restrictions I impose for IRFs on horizons 0 and $\infty$:


$$
\begin{align*}
\Theta_0 = B = B_0^{-1} &=
\begin{bmatrix}
* & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
* & * & 0 & 0 & 0 & 0 & 0 & 0 \\
* & * & * & 0 & * & 0 & 0 & 0 \\
* & * & * & * & 0 & 0 & 0 & 0 \\
* & * & * & * & * & 0 & 0 & 0 \\
* & * & * & * & * & * & 0 & 0 \\
* & * & * & * & * & * & * & 0 \\
* & * & * & * & * & * & * & * \\
\end{bmatrix}
 \\
\Theta_{\infty} = (B_0-B_1-B_2-B_3-B_4)^{-1} &=
\begin{bmatrix}
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & 0 & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
\end{bmatrix}
\end{align*}
$$

Where the structural form model is $QB_0y_t=Qb_0+QB_1y_{t-1}+QB_2y_{t-2}+QB_3y_{t-3}+QB_3y_{t-4}+Qu_t,  \quad u_t|Y_{t-1} \sim iid(0_8, I_8)$


## Normalization and Q matrix
To apply a long-run restriction, I need a rotation matrix for it. Obtaining the rotation matrix under the algorithm in Rubio-Ramírez, Waggoner & Zha (2010) requires a contemporaneous relationship matrix $B_0$ such that the number of restrictions in each column is weakly decreasing from the left to the right. Thus, I transpose the contemporaneous relationship matrix $B_0$ as follows:

$$
\begin{align*}
B_0' &=
\begin{bmatrix}
* & * & * & * & * & * & * & * \\
0 & * & * & * & * & * & * & * \\
0 & 0 & * & * & * & * & * & * \\
0 & 0 & 0 & * & * & * & * & * \\
0 & 0 & * & 0 & * & * & * & * \\
0 & 0 & 0 & 0 & 0 & * & * & * \\
0 & 0 & 0 & 0 & 0 & 0 & * & * \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & * \\
\end{bmatrix}
 \\
\Theta_{\infty}' = (B_0-B_1-B_2-B_3-B_4)' &=
\begin{bmatrix}
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & 0 & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
* & * & * & * & * & * & * & * \\
\end{bmatrix}
\end{align*}
$$
 

```{r results for case 2: long-run restriction}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from ECOM90007 L13 codes provided by Tomasz Woźniak.


# obtain Bplus.posterior.tilde and Binf = B0-B1-B2-B3-B4
############################################################
B0.posterior.tilde               = array(NA,c(N,N,S))
B0.posterior.transposed.tilde    = array(NA,c(N,N,S))
B0.posterior                     = array(NA,c(N,N,S))
B0.posterior.norm                = array(NA,c(N,N,S))
Bplus.posterior                  = array(NA,c(N,N*p+1,S))
Bplus.posterior.tilde            = array(NA,c(N,N*p+1,S))
IR.infinity                      = array(NA,c(N,N,S))
IR.infinity.transposed.tilde     = array(NA,c(N,N,S))
Q                                = array(NA,c(N,N,S))


```


```{r results for case 2: long-run restriction_contd2}
#| echo: true
#| message: false


# Initial setup
for (s in 1:S){
B0.posterior.tilde[,,s]       = solve(B.posterior[,,s])
Bplus.posterior.tilde[,,s]    = B0.posterior.tilde[,,s]%*%t(A.posterior[,,s])
  }

# transpose so that the ranks of restrictions are in descending order.
for (s in 1:S){
B0.posterior.transposed.tilde[,,s]      = t(B0.posterior.tilde[,,s])
IR.infinity.transposed.tilde[,,s]       = t(B0.posterior.tilde[,,s]-Bplus.posterior.tilde[,2:9,s]-Bplus.posterior.tilde[,10:17,s]-Bplus.posterior.tilde[,18:25,s]-Bplus.posterior.tilde[,26:33,s])
  }


# Orthogonal complement matrix
############################################################
orthogonal.complement.matrix.TW = function(x){
  # x is a mxn matrix and m>n
  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:
  # t(x)%*%out = 0 and det(cbind(x,out))!=0
  NN     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(NN[2]+1):NN[1]])
  return(out)
}


# Use Algorithm 1 to obtain Q
############################################################
R1 = cbind(matrix(0,7,1), diag(7), matrix(0,7,8))
R2 = cbind(matrix(0,6,2), diag(6), matrix(0,6,8))
R3 = matrix(0, 5,16)
R3[1,4] = 1
R3[2,6] = 1
R3[3,7] = 1
R3[4,8] = 1
R3[5,13] = 1
R4 = cbind(matrix(0,4,4), diag(4), matrix(0,4,8))
R5 = cbind(matrix(0,3,5), diag(3), matrix(0,3,8))
R6 = cbind(matrix(0,2,6), diag(2), matrix(0,2,8))
R7 = cbind(matrix(0,1,7), diag(1), matrix(0,1,8))



for (s in 1:S){
fAA             = rbind(B0.posterior.transposed.tilde[,,s],IR.infinity.transposed.tilde[,,s])
p1.s            = orthogonal.complement.matrix.TW(t(R1 %*% fAA))
p2.s            = orthogonal.complement.matrix.TW(cbind(t(R2 %*% fAA),p1.s))
p3.s            = orthogonal.complement.matrix.TW(cbind(t(R3 %*% fAA),p1.s,p2.s))
p4.s            = orthogonal.complement.matrix.TW(cbind(t(R4 %*% fAA),p1.s,p2.s,p3.s))
p5.s            = orthogonal.complement.matrix.TW(cbind(t(R5 %*% fAA),p1.s,p2.s,p3.s,p4.s))
p6.s            = orthogonal.complement.matrix.TW(cbind(t(R6 %*% fAA),p1.s,p2.s,p3.s,p4.s,p5.s))
p7.s            = orthogonal.complement.matrix.TW(cbind(t(R7 %*% fAA),p1.s,p2.s,p3.s,p4.s,p5.s,p6.s))
p8.s            = orthogonal.complement.matrix.TW(cbind(p1.s,p2.s,p3.s,p4.s,p5.s,p6.s,p7.s))

Q[,,s]            = cbind(p1.s,p2.s,p3.s,p4.s,p5.s,p6.s,p7.s,p8.s)

B0.posterior[,,s]    = Q[,,s]%*%B0.posterior.tilde[,,s]
Bplus.posterior[,,s] = Q[,,s]%*%Bplus.posterior.tilde[,,s]  ######!!@@@
IR.infinity[,,s]     = solve(B0.posterior[,,s]-Bplus.posterior[,2:9,s]-Bplus.posterior[,10:17,s]-Bplus.posterior[,18:25,s]-Bplus.posterior[,26:33,s])

}

```

The posterior $B_0$ matrix requires a normalization since it exhibits symmetric distribution round zero. We can additionally confirm this by looking at the IRF plots in Figure 7. Below is the normalization function for this.

```{r results for case 2: long-run restriction_contd2, normalization}
#| echo: true
#| message: false


# initial setup

# create diag.signs matrix
#1.Function to convert decimal to binary with leading zeros
decimal_to_binary <- function(decimal, length) {
  binary <- numeric(length)
  for (i in length:1) {
    binary[i] <- decimal %% 2
    decimal <- decimal %/% 2
  }
  return(binary)
}

# 2.Generate the matrix with 256 rows and 8 columns
diag.signs <- matrix(0, nrow = 256, ncol = 8)

# 3.Populate the matrix with unique combinations of -1 and 1
for (i in 0:255) {
  binary <- decimal_to_binary(i, 8)
  binary[binary == 0] <- -1
  diag.signs[i + 1, ] <- binary
}


# Function
normalization.wz2003  = function(B0.posterior.tmp,B0.posterior.hat.inv, Sigma.posterior.inv, diag.signs){
  # This function normalizes a matrix of contemporaneous effects
  # according to the algorithm by Waggoner & Zha (2003, JOE)
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.posterior.tmp)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0.posterior.tmp)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.posterior.hat.inv)[n,] %*%Sigma.posterior.inv %*% t(B0.tmp.inv - B0.posterior.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0.posterior.tmp
  
  return(B0.out)
}

#Obtain normalized B0
t0=  proc.time()
ttt=1
B0.posterior.hat.inv = -solve(B0.posterior[,,1]) # does - reverse the IRF?
for (s in 1:S){
Sigma.posterior.inv = solve(Sigma.posterior[,,s])
B0.posterior.tmp    = B0.posterior[,,s]

B0.posterior.norm[,,s] = normalization.wz2003(B0.posterior.tmp,B0.posterior.hat.inv, Sigma.posterior.inv, diag.signs)
ttt=ttt+s/s
}
t1          = proc.time()
(t1-t0)[3]/60 # Time of computations in minutes

```


## Empirical Investigation for model with long-run restriction (case 2)

Unlike the baseline model, the relationship between Australian real GDP and cash rate in the long-run is restricted such that there cash rate has no effect on Australian real GDP. Though this alternative restriction, I expect that the result would not be quite different from the result of the baseline model. This is because if I look at the IRF of Australian real GDP to cash rate, the effect is not different from zero statistically.

```{r results for case 2: long-run restriction_IRF, FEVD}
#| echo: false
#| message: false
#| warning: false
#| results: hide


# Impulse response functions
# Forecast Error Variance Decomposition
############################################################

t0          = proc.time()

LRR.IRF.posterior     = array(NA,c(N,N,h+1,S))
LRR.IRF.inf.posterior = array(NA,c(N,N,S))
LRR.FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                     = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S){
  LRR.A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  LRR.IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-LRR.A.bold) %*% t(J) %*% solve(B0.posterior[,,s])
  LRR.A.bold.power    = LRR.A.bold
  for (i in 1:(h+1)){
    LRR.tmp = solve(B0.posterior[,,s])
    if (i==1){
      LRR.IRF.posterior[,,i,s]        = LRR.tmp
    } else {
      LRR.IRF.posterior[,,i,s]        = J %*% LRR.A.bold.power %*% t(J) %*% LRR.tmp
      LRR.A.bold.power                = LRR.A.bold.power %*% LRR.A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        LRR.FEVD.posterior[n,nn,i,s]  = sum(LRR.IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    LRR.FEVD.posterior[,,i,s]         = diag(1/apply(LRR.FEVD.posterior[,,i,s],1,sum))%*%LRR.FEVD.posterior[,,i,s]
  }
}
LRR.FEVD.posterior    = 100*LRR.FEVD.posterior

t1          = proc.time()
(t1-t0)[3]/60 # Time of computations in minutes

save(LRR.IRF.posterior,LRR.IRF.inf.posterior, LRR.FEVD.posterior, file="irf-fevd-k1.RData3")


# Plots of responses to domestic monetary policy shock
############################################################
load("irf-fevd-k1.RData3")
LRR.IRFs.k1           = apply(LRR.IRF.posterior[3:8,5,,],1:2,mean)
LRR.IRFs.inf.k1       = apply(LRR.IRF.inf.posterior[3:8,5,],1,mean)
rownames(LRR.IRFs.k1) = colnames(Y)[3:8]

LRR.IRFs.k1.hdi    = apply(LRR.IRF.posterior[3:8,5,,],1:2,hdi, credMass=0.68)
hh             = 1:11



```

Since it is before normalization, it shows some weird plots as below.


###### Figure 7. IRF to cash rate shock with long-run restiction before normalization {style="text-align: center;"}

```{r irf-au-mps plots3, echo = FALSE}

# irf-au-mps plots

par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:6){
  ylims     = range(LRR.IRFs.k1[n,hh],LRR.IRFs.k1.hdi[,n,hh])
  plot(hh,LRR.IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(LRR.IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,2,6,11),c("","1 quarter","5 quarter","10 quarter"))
  } else {
    axis(1,c(1,2,6,11),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(LRR.IRFs.k1.hdi[1,n,hh],LRR.IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, LRR.IRFs.k1[n,hh],lwd=2,col=mcxs1)
  
}

```


###### Figure 8. FEVD of banks' credit plots with long-run restiction before normalization {style="text-align: center;"}


```{r fevd-credit plots3, echo = FALSE}
# Plots of FEVD of Australian banks credit
############################################################
load("irf-fevd-k1.RData3")
hh            = 1:(h+1)
LRR.fevd.au.credit= apply(LRR.FEVD.posterior[7,,,],1:2,mean)
LRR.fevd.au.credit= rbind(rep(0,h+1),apply(LRR.fevd.au.credit,2,cumsum))

colors = c("deepskyblue1","deepskyblue2","maroon1","maroon","maroon2","magenta","maroon3","maroon4")


par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,LRR.fevd.au.credit[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
axis(1,hh,c("","1 quarter","","","","5 quarter","","","","","10 quarter"))
axis(2,c(0,50,100),c("","FEVD[au.credit]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(LRR.fevd.au.credit[n,hh],LRR.fevd.au.credit[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(LRR.fevd.au.credit[1:8,7]+LRR.fevd.au.credit[2:9,7]))[c(5,7)], c("crd", "mps"))

```



```{r results for case 2: long-run restriction_IRF, FEVD_normalized}
#| echo: false
#| message: false
#| warning: false
#| results: hide


# Impulse response functions
# Forecast Error Variance Decomposition
############################################################

t0          = proc.time()

norm.LRR.IRF.posterior     = array(NA,c(N,N,h+1,S))
norm.LRR.IRF.inf.posterior = array(NA,c(N,N,S))
norm.LRR.FEVD.posterior    = array(NA,c(N,N,h+1,S))
norm.J                     = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S){
  norm.LRR.A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  norm.LRR.IRF.inf.posterior[,,s]          = norm.J %*% solve(diag(N*p)-norm.LRR.A.bold) %*% t(norm.J) %*% solve(B0.posterior.norm[,,s])
  norm.LRR.A.bold.power    = norm.LRR.A.bold
  for (i in 1:(h+1)){
    norm.LRR.tmp = solve(B0.posterior.norm[,,s])
    if (i==1){
      norm.LRR.IRF.posterior[,,i,s]        = norm.LRR.tmp
    } else {
      norm.LRR.IRF.posterior[,,i,s]        = norm.J %*% norm.LRR.A.bold.power %*% t(norm.J) %*% norm.LRR.tmp
      norm.LRR.A.bold.power                = norm.LRR.A.bold.power %*% norm.LRR.A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        norm.LRR.FEVD.posterior[n,nn,i,s]  = sum(norm.LRR.IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    norm.LRR.FEVD.posterior[,,i,s]         = diag(1/apply(norm.LRR.FEVD.posterior[,,i,s],1,sum))%*%norm.LRR.FEVD.posterior[,,i,s]
  }
}
norm.LRR.FEVD.posterior    = 100*norm.LRR.FEVD.posterior

t1          = proc.time()
(t1-t0)[3]/60 # Time of computations in minutes

save(norm.LRR.IRF.posterior,norm.LRR.IRF.inf.posterior, norm.LRR.FEVD.posterior, file="irf-fevd-k1.RData4")


# Plots of responses to domestic monetary policy shock
############################################################
load("irf-fevd-k1.RData4")
norm.LRR.IRFs.k1           = apply(norm.LRR.IRF.posterior[3:8,5,,],1:2,mean)
norm.LRR.IRFs.inf.k1       = apply(norm.LRR.IRF.inf.posterior[3:8,5,],1,mean)
rownames(norm.LRR.IRFs.k1) = colnames(Y)[3:8]

norm.LRR.IRFs.k1.hdi    = apply(norm.LRR.IRF.posterior[3:8,5,,],1:2,hdi, credMass=0.68)
hh             = 1:11



```
#

Below is IRF with long-run restiction and normalization.Though the point estimations deviate from zero but most of the confident intervals of variables include zero except real GDP which decrease but it gets to zero statistically as time proceeds.

###### Figure 9. IRF to cash rate shock with long-run restiction and normalization {style="text-align: center;"}

```{r irf-au-mps plots3_normalized, echo = FALSE}

# irf-au-mps plots

par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:6){
  ylims     = range(norm.LRR.IRFs.k1[n,hh],norm.LRR.IRFs.k1.hdi[,n,hh])
  plot(hh,norm.LRR.IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(norm.LRR.IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,2,6,11),c("","1 quarter","5 quarter","10 quarter"))
  } else {
    axis(1,c(1,2,6,11),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(norm.LRR.IRFs.k1.hdi[1,n,hh],norm.LRR.IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, norm.LRR.IRFs.k1[n,hh],lwd=2,col=mcxs1)
  
}

```








# Identiying assumptions: case 3 (7 variables without deposit)
```{r, warning=FALSE, message=FALSE}
#| echo: false
#| message: false
#| warning: false
#| results: hide
# dm_us_ex_rate, dm_cashrate are original, others are log transformed.
y2          = cbind(log_merged_data[,1],
                    log_merged_data[,2],
                    log_merged_data[,3],
                    log_merged_data[,4],
                        merged_data[,8],
                    log_merged_data[,5],
                        merged_data[,7]
                    )

colnames(y2) <- c("commodity price", "usgdp", "GDP", "CPI", "Cash rate", "Bank credit", "AUD/USD ER")

```

In case 3, I shut down the deposit channel to see how banks' credit respond upon cash rate shock. Following equation summarizes the contemporaneous relationships among the variables in this case:

$$
B_0y_t =
\begin{bmatrix}
a_{11} & 0 & 0 & 0 & 0 & 0 & 0 \\
a_{21} & a_{22} & 0 & 0 & 0 & 0 & 0 \\
a_{31} & a_{32} & a_{33} & 0 & 0 & 0 & 0 \\
a_{41} & a_{42} & a_{43} & a_{44} & 0 & 0 & 0 \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} & 0 & 0 \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} & a_{66} & 0 \\
a_{71} & a_{72} & a_{73} & a_{74} & a_{75} & a_{76} & a_{77}\\
\end{bmatrix}
\begin{bmatrix}
com_t \\
usgdp_t \\
gdp_t \\
\pi_t \\
i_t \\
credit_t \\
ex_t \\
\end{bmatrix}
$$





```{r results for case 3}
#| echo: false
#| message: false
#| warning: false
#| results: hide

# This code is from ECOM90007 L12 codes provided by Tomasz Woźniak.

library(mvtnorm)
library(plot3D)
library(HDInterval)
set.seed(123456)


# setup
############################################################
N2      = 7
# p, s, h are the same as in the basline model

# create Y and X
############################################################
y2      = ts(y2)
Y2       = ts(y2[5:118,], start=c(1991,1), frequency=4)
X2       = matrix(1,nrow(Y2),1)
for (i in 1:p){
  X2     = cbind(X2,y2[5:118-i,])
}


t0          = proc.time() # read processor time

# MLE
############################################################
V7.A.hat       = solve(t(X2)%*%X2)%*%t(X2)%*%Y2
V7.Sigma.hat   = t(Y2-X2%*%V7.A.hat)%*%(Y2-X2%*%V7.A.hat)/nrow(Y2)


# prior distribution
############################################################
#Kappa 1,2,3 are the same as in the baseline model

V7.A.prior        = matrix(0,nrow(V7.A.hat),ncol(V7.A.hat))
V7.A.prior[2:8,]  = kappa.3*diag(N2)
V7.V.prior        = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N2)))
V7.S.prior        = diag(diag(V7.Sigma.hat))
V7.nu.prior       = N2+1

# normal-inverse Wishard posterior parameters
############################################################
V7.V.bar.inv   = t(X2)%*%X2 + diag(1/diag(V7.V.prior))
V7.V.bar       = solve(V7.V.bar.inv)
V7.A.bar       = V7.V.bar%*%(t(X2)%*%Y2 + diag(1/diag(V7.V.prior))%*%V7.A.prior)
V7.nu.bar      = nrow(Y2) + V7.nu.prior
V7.S.bar       = V7.S.prior + t(Y2)%*%Y2 + t(V7.A.prior)%*%diag(1/diag(V7.V.prior))%*%V7.A.prior - t(V7.A.bar)%*%V7.V.bar.inv%*%V7.A.bar
V7.S.bar.inv   = solve(V7.S.bar)

# posterior draws 
############################################################
V7.Sigma.posterior   = rWishart(S, df=V7.nu.bar, Sigma=V7.S.bar.inv)
V7.Sigma.posterior   = apply(V7.Sigma.posterior,3,solve)
V7.Sigma.posterior   = array(V7.Sigma.posterior,c(N2,N2,S))
V7.A.posterior       = array(rnorm(prod(c(dim(V7.A.bar),S))),c(dim(V7.A.bar),S))
V7.B.posterior       = array(NA,c(N2,N2,S))
V7.L                 = t(chol(V7.V.bar))
for (s in 1:S){
  V7.cholSigma.s     = chol(V7.Sigma.posterior[,,s])
  V7.B.posterior[,,s]= t(V7.cholSigma.s)
  V7.A.posterior[,,s]= V7.A.bar + V7.L%*%V7.A.posterior[,,s]%*%V7.cholSigma.s
}



# Impulse response functions
# Forecast Error Variance Decomposition
############################################################
V7.IRF.posterior     = array(NA,c(N2,N2,h+1,S))
V7.IRF.inf.posterior = array(NA,c(N2,N2,S))
V7.FEVD.posterior    = array(NA,c(N2,N2,h+1,S))
V7.J                 = cbind(diag(N2),matrix(0,N2,N2*(p-1)))
for (s in 1:S){
  V7.A.bold          = rbind(t(V7.A.posterior[2:(1+N2*p),,s]),cbind(diag(N2*(p-1)),matrix(0,N2*(p-1),N2)))
  V7.IRF.inf.posterior[,,s]          = V7.J %*% solve(diag(N2*p)-V7.A.bold) %*% t(V7.J) %*% V7.B.posterior[,,s]
  V7.A.bold.power    = V7.A.bold
  for (i in 1:(h+1)){
    V7.tmp = V7.B.posterior[,,s]
    if (i==1){
      V7.IRF.posterior[,,i,s]        = V7.tmp
    } else {
      V7.IRF.posterior[,,i,s]        = V7.J %*% V7.A.bold.power %*% t(V7.J) %*% V7.tmp
      V7.A.bold.power                = V7.A.bold.power %*% V7.A.bold
    }
    for (n in 1:N2){
      for (nn in 1:N2){
        V7.FEVD.posterior[n,nn,i,s]  = sum(V7.IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    V7.FEVD.posterior[,,i,s]         = diag(1/apply(V7.FEVD.posterior[,,i,s],1,sum))%*%V7.FEVD.posterior[,,i,s]
  }
}
V7.FEVD.posterior    = 100*V7.FEVD.posterior

t1          = proc.time()
(t1-t0)[3]/60 # Time of computations in minutes

save(V7.IRF.posterior,V7.IRF.inf.posterior, V7.FEVD.posterior, file="irf-fevd-k1.RData2")


# Plots of responses to domestic monetary policy shock
############################################################
load("irf-fevd-k1.RData2")
V7.IRFs.k1           = apply(V7.IRF.posterior[3:7,5,,],1:2,mean)
V7.IRFs.inf.k1       = apply(V7.IRF.inf.posterior[3:7,5,],1,mean)
rownames(V7.IRFs.k1) = colnames(Y2)[3:7]

V7.IRFs.k1.hdi    = apply(V7.IRF.posterior[3:7,5,,],1:2,hdi, credMass=0.68)
hh             = 1:11
```


## Empirical Investigation for model without deposit (case 3)
As we see the figure of response function of banks credit to cash rate shock, banks credit decreases less upon a positive cash rate shock in the model without banks' deposit. Another interesting observation is that GDP also decreases less under this model than it did in baseline model in which deposit is included. In other words, GDP, a real economy variable, and credit, a financial economy variable are both responding more sensitively to monetary policy shock when deposit is included in the model. 



###### Figure 10. IRF to cash rate shock for $\kappa_1$ = 1 plots {style="text-align: center;"}

```{r irf-au-mps plots2, echo = FALSE}

# irf-au-mps plots

par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:5){
  ylims     = range(V7.IRFs.k1[n,hh],V7.IRFs.k1.hdi[,n,hh])
  plot(hh,V7.IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(V7.IRFs.k1)[n])
  if (n==4 | n==5){
    axis(1,c(1,2,6,11),c("","1 quarter","5 quarter","10 quarter"))
  } else {
    axis(1,c(1,2,6,11),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(V7.IRFs.k1.hdi[1,n,hh],V7.IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, V7.IRFs.k1[n,hh],lwd=2,col=mcxs1)
  
}
```


###### Figure 11. FEVD of banks' credit plots {style="text-align: center;"}


```{r fevd-credit plots2, echo = FALSE}
# Plots of FEVD of Australian banks credit
############################################################
load("irf-fevd-k1.RData2")
hh            = 1:(h+1)
V7.fevd.au.credit= apply(V7.FEVD.posterior[6,,,],1:2,mean)
V7.fevd.au.credit= rbind(rep(0,h+1),apply(V7.fevd.au.credit,2,cumsum))

colors = c("deepskyblue1","deepskyblue2","maroon1","maroon","maroon2","magenta","maroon3")


par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,V7.fevd.au.credit[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
axis(1,hh,c("","1 quarter","","","","5 quarter","","","","","10 quarter"))
axis(2,c(0,50,100),c("","FEVD[au.credit]",""))
for (n in 1:N2){
  polygon(c(hh,(h+1):1), c(V7.fevd.au.credit[n,hh],V7.fevd.au.credit[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(V7.fevd.au.credit[1:7,6]+V7.fevd.au.credit[2:8,6]))[c(5,6)], c("mps","crds"))
```

# Stochastic volatility heteroskedasticity

## Model equations
$$
Y = X\alpha + E
$$
$$
E|X \sim MN(0_{T\times N},\Sigma,diag(h))
$$

$$
h = (h_1,...,h_T)'
$$
$$
\sigma^2 = (exp\{h_1\}, ... , exp\{h_t\})
$$
$h_t$ - follows a Stochastic Volatility process

The likelihood function is then
$$
L(\alpha,\Sigma|Y,X,h) = det(diag(h)^{-N/2}det(\Sigma)^{-T/2}\times exp\{  -1/2 [\Sigma ^ {-1} (Y-\alpha X)' diag(h)^{-1}(Y-\alpha X)] \}
$$
The full conditional posterior distribution for $\alpha,\Sigma$ is given by
$$
P(\alpha,\Sigma|Y,X,h) = MN (\bar{\alpha},\bar{V},\bar{S},\bar{\nu})
$$
where
$$
\bar{V}=(X'diag(h)^{-1}X+\underline{V}^{-1})^{-1}
$$
$$
\bar{\alpha}=\bar{V}(X'diag(h)^{-1}Y+\underline{V}^{-1}\underline{\alpha})
$$
$$
\bar{S}=\underline{V}+Y'diag(h)^{-1}Y+\underline{\alpha'}\underline{V}^{-1}\underline{\alpha}-\bar{\alpha'}\bar{V}^{-1}\bar{\alpha}
$$
$$
\bar{\nu}= T + \underline{\nu}
$$


## Gibbs sampler

At each iteration s:

   Step 1. Sample $h^{S}$ from the distribution $P(h|Y,X,A, \Sigma)$
   Step 2. Sample $\alpha^{S},\Sigma^{s}$ from the $P(A,\Sigma|Y,X,h) = MN (\bar{\alpha},\bar{V},\bar{S},\bar{\nu})$
   
Return $\{\alpha^{S},\Sigma^{S},h^{S}]\}_{s=1}^S$

Technics for sampling h is listed in the previous section.

Then it follows that 
$\bar{V} = (X'diag(h)^{-1}X + \underline{V}^{-1})^{-1}$
$\bar{A} = \bar{V}(X'diag(h)^{-1}Y + \underline{V}^{-1}\underline{A})^{-1}$
$\bar{S} = \underline{S} + Y'diag(h)^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}$
$\bar{\nu}=T + \underline{\nu}$.


## Algorithm

Following is the algorithm for the part above to sample draws from full conditional posterior distribution. For each iteration, it draws h from a corresponding distribution and then using h, and then draws A and sigma from a multivariate normal distribution.

```{r setup, include=TRUE, cache = FALSE}
#| eval: false
#| echo: true

SVcommon.Gibbs.iteration = function(S,aux, priors){
  

  #
  # aux is a list containing:
  #   Y - a TxN matrix
  #   X - a TxK matrix
  #   H - a Tx1 matrix
  #   h0 - a scalar
  #   sigma.v2 - a scalar
  #   s - a Tx1 matrix
  #   A - a KxN matrix
  #   Sigma - an NxN matrix
  #   sigma2 - a Tx1 matrix
  #
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix

  
  T             = dim(aux$Y)[1]
  N             = dim(aux$Y)[2]
  K             = dim(X)[2]
  

    
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  

  Lambda        = solve(chol(aux$Sigma))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
  
    # Sampling A and Sigma
  kappa.1     = 1
  kappa.2     = 100
  kappa.3     = 1

  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))

  nu.prior    = N+1
  # normal-inverse Wishard posterior parameters
  ############################################################
  V.bar.inv   = t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(aux$X)%*%aux$Y + diag(1/diag(V.prior))%*%aux$A)
  nu.bar      = nrow(aux$Y) + nu.prior
  S.bar       = aux$Sigma + t(aux$Y)%*%aux$Y + t(A.prior)%*%diag(1/diag(V.prior))%*%aux$A - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################

  Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)
  aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))
  A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))
  L                      = t(chol(V.bar))
  aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])
  # sampling initial condition
  ############################################################
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        = h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  = sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s         = as.matrix(ss)
  
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H         = as.matrix(draw)
  aux$sigma2    = as.matrix(exp(draw))
  


  
  posteriors      = list(
    H           = matrix(NA,T,S),
    sigma2      = matrix(NA,T,S),
    s           = matrix(NA,T,S),
    h0          = rep(NA,S),
    sigma.v2    = rep(NA,S),
    A           = array(NA, c(K,N,S)),
    Sigma       = array(NA, c(N,N,S))
  )

  for (s in 1:S){
      posteriors$H[,s]             = aux$H
      posteriors$sigma2[,s]        = aux$sigma2
      posteriors$s[,s]             = aux$s
      posteriors$h0[s]             = aux$h0
      posteriors$sigma.v2[s]       = aux$sigma.v2
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
  }
  

  return(posteriors)
}


```


# Estimation of hyperparameters
Estimating hyperparameters for the prior distribution from data can provide some advantages. By estimating the hyperparameters from data, we can expect that the prior distribution gets more data-driven since it incorporates information specific to the observed dataset. It can lead to more accurate and informative prior for the model. Also, potentially the overall fit of the model can be improved as well.


#  Conclusion
This research aimed to investigate the relationship between monetary policy, banks' deposits, and credit growth in Australia. The objective was to understand how changes in monetary policy influence  banks' credit together with deposit growth. The study incorporated the deposit channel proposed by @drechsler2017deposits in the SVAR framework to evaluate the effect of monetary policy with deposit changes on banks' credit. The findings indicate that when deposits are included in the model, banks' credit decreases more in response to a positive cash rate shock, and GDP exhibits a larger decrease compared to the model without the deposit. These observations highlight the importance of considering the deposit channel in understanding the transmission of monetary policy and its impact on both real and financial economy variables.



# References {.unnumbered}

